{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sonic-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install googledrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "behind-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "# gdd.download_file_from_google_drive(file_id='10ZkXiH8eWGcoPdKazYk6JApOlfepz2oN',\n",
    "#                                     dest_path='./data/speed_matrix_2015.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-accounting",
   "metadata": {},
   "source": [
    "## GRU-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "drawn-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat May 12 16:48:54 2018\n",
    "\n",
    "@author: Zhiyong\n",
    "\n",
    "@article{che2018recurrent,\n",
    "  title={Recurrent neural networks for multivariate time series with missing values},\n",
    "  author={Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan},\n",
    "  journal={Scientific reports},\n",
    "  volume={8},\n",
    "  number={1},\n",
    "  pages={6085},\n",
    "  year={2018},\n",
    "  publisher={Nature Publishing Group}\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch.utils.data as utils\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class FilterLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, filter_square_matrix, bias=True):\n",
    "        '''\n",
    "        filter_square_matrix : filter square matrix, whose each elements is 0 or 1.\n",
    "        '''\n",
    "        super(FilterLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        self.filter_square_matrix = None\n",
    "        if use_gpu:\n",
    "            self.filter_square_matrix = Variable(filter_square_matrix.cuda(), requires_grad=False)\n",
    "        else:\n",
    "            self.filter_square_matrix = Variable(filter_square_matrix, requires_grad=False)\n",
    "        \n",
    "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "#         print(self.weight.data)\n",
    "#         print(self.bias.data)\n",
    "\n",
    "    def forward(self, input):\n",
    "#         print(self.filter_square_matrix.mul(self.weight))\n",
    "        return F.linear(input, self.filter_square_matrix.mul(self.weight), self.bias)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "            + 'in_features=' + str(self.in_features) \\\n",
    "            + ', out_features=' + str(self.out_features) \\\n",
    "            + ', bias=' + str(self.bias is not None) + ')'\n",
    "        \n",
    "class GRUD(nn.Module):\n",
    "    def __init__(self, input_size, cell_size, hidden_size, X_mean, output_last = False):\n",
    "        \"\"\"\n",
    "        Recurrent Neural Networks for Multivariate Times Series with Missing Values\n",
    "        GRU-D: GRU exploit two representations of informative missingness patterns, i.e., masking and time interval.\n",
    "        cell_size is the size of cell_state.\n",
    "        \n",
    "        Implemented based on the paper: \n",
    "        @article{che2018recurrent,\n",
    "          title={Recurrent neural networks for multivariate time series with missing values},\n",
    "          author={Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan},\n",
    "          journal={Scientific reports},\n",
    "          volume={8},\n",
    "          number={1},\n",
    "          pages={6085},\n",
    "          year={2018},\n",
    "          publisher={Nature Publishing Group}\n",
    "        }\n",
    "        \n",
    "        GRU-D:\n",
    "            input_size: variable dimension of each time\n",
    "            hidden_size: dimension of hidden_state\n",
    "            mask_size: dimension of masking vector\n",
    "            X_mean: the mean of the historical input data\n",
    "        \"\"\"\n",
    "        \n",
    "        super(GRUD, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.delta_size = input_size\n",
    "        self.mask_size = input_size\n",
    "        \n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        if use_gpu:\n",
    "            self.identity = torch.eye(input_size).cuda()\n",
    "            self.zeros = Variable(torch.zeros(input_size).cuda())\n",
    "            self.X_mean = Variable(torch.Tensor(X_mean).cuda())\n",
    "        else:\n",
    "            self.identity = torch.eye(input_size)\n",
    "            self.zeros = Variable(torch.zeros(input_size))\n",
    "            self.X_mean = Variable(torch.Tensor(X_mean))\n",
    "        \n",
    "        self.zl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size)\n",
    "        self.rl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size)\n",
    "        self.hl = nn.Linear(input_size + hidden_size + self.mask_size, hidden_size)\n",
    "        \n",
    "        self.gamma_x_l = FilterLinear(self.delta_size, self.delta_size, self.identity)\n",
    "        \n",
    "        self.gamma_h_l = nn.Linear(self.delta_size, self.delta_size)\n",
    "        \n",
    "        self.output_last = output_last\n",
    "        \n",
    "    def step(self, x, x_last_obsv, x_mean, h, mask, delta):\n",
    "        batch_size = x.shape[0]\n",
    "        dim_size = x.shape[1]\n",
    "        \n",
    "        delta_x = torch.exp(-torch.max(self.zeros, self.gamma_x_l(delta)))\n",
    "        delta_h = torch.exp(-torch.max(self.zeros, self.gamma_h_l(delta)))\n",
    "        \n",
    "        x = mask * x + (1 - mask) * (delta_x * x_last_obsv + (1 - delta_x) * x_mean)\n",
    "        h = delta_h * h\n",
    "        \n",
    "        combined = torch.cat((x, h, mask), 1)\n",
    "        z = F.sigmoid(self.zl(combined))\n",
    "        r = F.sigmoid(self.rl(combined))\n",
    "        combined_r = torch.cat((x, r * h, mask), 1)\n",
    "        h_tilde = F.tanh(self.hl(combined_r))\n",
    "        h = (1 - z) * h + z * h_tilde\n",
    "        \n",
    "        imputed_x = x\n",
    "        \n",
    "        return imputed_x, h\n",
    "    \n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(0)\n",
    "        type_size = input.size(1)\n",
    "        step_size = input.size(2)\n",
    "        spatial_size = input.size(3)\n",
    "        \n",
    "        Hidden_State = self.initHidden(batch_size)\n",
    "        X = torch.squeeze(input[:,0,:,:])\n",
    "        X_last_obsv = torch.squeeze(input[:,1,:,:])\n",
    "        Mask = torch.squeeze(input[:,2,:,:])\n",
    "        Delta = torch.squeeze(input[:,3,:,:])\n",
    "        \n",
    "        outputs = None\n",
    "        imputed_seq = []\n",
    "        for i in range(step_size):\n",
    "            imputed_x, Hidden_State = self.step(torch.squeeze(X[:,i:i+1,:])\\\n",
    "                                     , torch.squeeze(X_last_obsv[:,i:i+1,:])\\\n",
    "                                     , torch.squeeze(self.X_mean[:,i:i+1,:])\\\n",
    "                                     , Hidden_State\\\n",
    "                                     , torch.squeeze(Mask[:,i:i+1,:])\\\n",
    "                                     , torch.squeeze(Delta[:,i:i+1,:]))\n",
    "            imputed_seq.append(imputed_x)\n",
    "            if outputs is None:\n",
    "                outputs = Hidden_State.unsqueeze(1)\n",
    "            else:\n",
    "                outputs = torch.cat((outputs, Hidden_State.unsqueeze(1)), 1)\n",
    "\n",
    "        imputed_outputs = torch.reshape(torch.hstack(imputed_seq), (64, 10, -1))\n",
    "\n",
    "        if self.output_last:\n",
    "            return outputs[:,-1,:], imputed_outputs\n",
    "        else:\n",
    "            return outputs, imputed_outputs\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        if use_gpu:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
    "            return Hidden_State\n",
    "        else:\n",
    "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
    "            return Hidden_State\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-barrel",
   "metadata": {},
   "source": [
    "## MAIN.PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "centered-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat May 12 16:49:49 2018\n",
    "\n",
    "@author: Zhiyong\n",
    "\"\"\"\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "def PrepareDataset(speed_matrix, \\\n",
    "                   BATCH_SIZE = 40, \\\n",
    "                   seq_len = 10, \\\n",
    "                   pred_len = 1, \\\n",
    "                   train_propotion = 0.7, \\\n",
    "                   valid_propotion = 0.2, \\\n",
    "                   masking = False, \\\n",
    "                   mask_ones_proportion = 0.8):\n",
    "    \"\"\" Prepare training and testing datasets and dataloaders.\n",
    "    \n",
    "    Convert speed/volume/occupancy matrix to training and testing dataset. \n",
    "    The vertical axis of speed_matrix is the time axis and the horizontal axis \n",
    "    is the spatial axis.\n",
    "    \n",
    "    Args:\n",
    "        speed_matrix: a Matrix containing spatial-temporal speed data for a network\n",
    "        seq_len: length of input sequence\n",
    "        pred_len: length of predicted sequence\n",
    "    Returns:\n",
    "        Training dataloader\n",
    "        Testing dataloader\n",
    "    \"\"\"\n",
    "    time_len = speed_matrix.shape[0]\n",
    "    \n",
    "    speed_matrix = speed_matrix.clip(0, 100)\n",
    "    \n",
    "    max_speed = speed_matrix.max().max()\n",
    "    speed_matrix =  speed_matrix / max_speed\n",
    "    \n",
    "    speed_sequences, speed_labels = [], []\n",
    "    for i in range(time_len - seq_len - pred_len):\n",
    "        speed_sequences.append(speed_matrix.iloc[i:i+seq_len].values)\n",
    "        speed_labels.append(speed_matrix.iloc[i+seq_len:i+seq_len+pred_len].values)\n",
    "    speed_sequences, speed_labels = np.asarray(speed_sequences), np.asarray(speed_labels)\n",
    "    \n",
    "    # using zero-one mask to randomly set elements to zeros\n",
    "    if masking:\n",
    "        print('Split Speed finished. Start to generate Mask, Delta, Last_observed_X ...')\n",
    "        np.random.seed(1024)\n",
    "        Mask = np.random.choice([0,1], size=(speed_sequences.shape), p = [1 - mask_ones_proportion, mask_ones_proportion])\n",
    "        speed_sequences = np.multiply(speed_sequences, Mask)\n",
    "        \n",
    "        # temporal information\n",
    "        interval = 5 # 5 minutes\n",
    "        S = np.zeros_like(speed_sequences) # time stamps\n",
    "        for i in range(S.shape[1]):\n",
    "            S[:,i,:] = interval * i\n",
    "\n",
    "        Delta = np.zeros_like(speed_sequences) # time intervals\n",
    "        for i in range(1, S.shape[1]):\n",
    "            Delta[:,i,:] = S[:,i,:] - S[:,i-1,:]\n",
    "\n",
    "        missing_index = np.where(Mask == 0)\n",
    "\n",
    "        X_last_obsv = np.copy(speed_sequences)\n",
    "        for idx in trange(missing_index[0].shape[0]):\n",
    "            i = missing_index[0][idx] \n",
    "            j = missing_index[1][idx]\n",
    "            k = missing_index[2][idx]\n",
    "            if j != 0 and j != 9:\n",
    "                Delta[i,j+1,k] = Delta[i,j+1,k] + Delta[i,j,k]\n",
    "            if j != 0:\n",
    "                X_last_obsv[i,j,k] = X_last_obsv[i,j-1,k] # last observation\n",
    "        Delta = Delta / Delta.max() # normalize\n",
    "    \n",
    "    # shuffle and split the dataset to training and testing datasets\n",
    "    print('Generate Mask, Delta, Last_observed_X finished. Start to shuffle and split dataset ...')\n",
    "    sample_size = speed_sequences.shape[0]\n",
    "    index = np.arange(sample_size, dtype = int)\n",
    "    np.random.seed(1024)\n",
    "    np.random.shuffle(index)\n",
    "    \n",
    "    speed_sequences = speed_sequences[index]\n",
    "    speed_labels = speed_labels[index]\n",
    "    \n",
    "    if masking:\n",
    "        X_last_obsv = X_last_obsv[index]\n",
    "        Mask = Mask[index]\n",
    "        Delta = Delta[index]\n",
    "        speed_sequences = np.expand_dims(speed_sequences, axis=1)\n",
    "        X_last_obsv = np.expand_dims(X_last_obsv, axis=1)\n",
    "        Mask = np.expand_dims(Mask, axis=1)\n",
    "        Delta = np.expand_dims(Delta, axis=1)\n",
    "        dataset_agger = np.concatenate((speed_sequences, X_last_obsv, Mask, Delta), axis = 1)\n",
    "        \n",
    "    train_index = int(np.floor(sample_size * train_propotion))\n",
    "    valid_index = int(np.floor(sample_size * ( train_propotion + valid_propotion)))\n",
    "    \n",
    "    if masking:\n",
    "        train_data, train_label = dataset_agger[:train_index], speed_labels[:train_index]\n",
    "        valid_data, valid_label = dataset_agger[train_index:valid_index], speed_labels[train_index:valid_index]\n",
    "        test_data, test_label = dataset_agger[valid_index:], speed_labels[valid_index:]\n",
    "    else:\n",
    "        train_data, train_label = speed_sequences[:train_index], speed_labels[:train_index]\n",
    "        valid_data, valid_label = speed_sequences[train_index:valid_index], speed_labels[train_index:valid_index]\n",
    "        test_data, test_label = speed_sequences[valid_index:], speed_labels[valid_index:]\n",
    "    \n",
    "    train_data, train_label = torch.Tensor(train_data), torch.Tensor(train_label)\n",
    "    valid_data, valid_label = torch.Tensor(valid_data), torch.Tensor(valid_label)\n",
    "    test_data, test_label = torch.Tensor(test_data), torch.Tensor(test_label)\n",
    "    \n",
    "    train_dataset = utils.TensorDataset(train_data, train_label)\n",
    "    valid_dataset = utils.TensorDataset(valid_data, valid_label)\n",
    "    test_dataset = utils.TensorDataset(test_data, test_label)\n",
    "    \n",
    "    train_dataloader = utils.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last = True)\n",
    "    valid_dataloader = utils.DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last = True)\n",
    "    test_dataloader = utils.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=True, drop_last = True)\n",
    "    \n",
    "    X_mean = np.mean(speed_sequences, axis = 0)\n",
    "    \n",
    "    print('Finished')\n",
    "    \n",
    "    return train_dataloader, valid_dataloader, test_dataloader, max_speed, X_mean\n",
    "\n",
    "\n",
    "\n",
    "def Train_Model(model, train_dataloader, valid_dataloader, num_epochs = 300, patience = 10, min_delta = 0.00001):\n",
    "    \n",
    "    print('Model Structure: ', model)\n",
    "    print('Start Training ... ')\n",
    "    \n",
    "    model.cuda()\n",
    "    \n",
    "    if (type(model) == nn.modules.container.Sequential):\n",
    "        output_last = model[-1].output_last\n",
    "        print('Output type dermined by the last layer')\n",
    "    else:\n",
    "        output_last = model.output_last\n",
    "        print('Output type dermined by the model')\n",
    "        \n",
    "    loss_MSE = torch.nn.MSELoss()\n",
    "    loss_L1 = torch.nn.L1Loss()\n",
    "    \n",
    "    learning_rate = 0.0001\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr = learning_rate, alpha=0.99)\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    \n",
    "    interval = 100\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "    losses_epochs_train = []\n",
    "    losses_epochs_valid = []\n",
    "    \n",
    "    cur_time = time.time()\n",
    "    pre_time = time.time()\n",
    "    \n",
    "    # Variables for Early Stopping\n",
    "    is_best_model = 0\n",
    "    patient_epoch = 0\n",
    "    for epoch in trange(num_epochs):\n",
    "        \n",
    "        trained_number = 0\n",
    "        \n",
    "        valid_dataloader_iter = iter(valid_dataloader)\n",
    "        \n",
    "        losses_epoch_train = []\n",
    "        losses_epoch_valid = []\n",
    "        \n",
    "        for data in train_dataloader:\n",
    "            inputs, labels = data\n",
    "#             print(\">> raw input : \", inputs[:,0,...]) # torch.Size([64, 10, 323])\n",
    "            if inputs.shape[0] != batch_size:\n",
    "                continue\n",
    "\n",
    "            if use_gpu:\n",
    "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            else: \n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            model.zero_grad()\n",
    "\n",
    "            outputs, imputed_outputs = model(inputs)\n",
    "            \n",
    "#             print(\">> imputed out\", imputed_outputs.shape)\n",
    "#             print(imputed_outputs)\n",
    "            \n",
    "            if output_last:\n",
    "                loss_train = loss_MSE(torch.squeeze(outputs), torch.squeeze(labels))\n",
    "            else:\n",
    "                full_labels = torch.cat((inputs[:,1:,:], labels), dim = 1)\n",
    "                loss_train = loss_MSE(outputs, full_labels)\n",
    "        \n",
    "            losses_train.append(loss_train.data)\n",
    "            losses_epoch_train.append(loss_train.data)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss_train.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "             # validation \n",
    "            try: \n",
    "                inputs_val, labels_val = next(valid_dataloader_iter)\n",
    "            except StopIteration:\n",
    "                valid_dataloader_iter = iter(valid_dataloader)\n",
    "                inputs_val, labels_val = next(valid_dataloader_iter)\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs_val, labels_val = Variable(inputs_val.cuda()), Variable(labels_val.cuda())\n",
    "            else: \n",
    "                inputs_val, labels_val = Variable(inputs_val), Variable(labels_val)\n",
    "                \n",
    "            model.zero_grad()\n",
    "            \n",
    "            outputs_val, imputed_outputs = model(inputs_val)\n",
    "            \n",
    "            if output_last:\n",
    "                loss_valid = loss_MSE(torch.squeeze(outputs_val), torch.squeeze(labels_val))\n",
    "            else:\n",
    "                full_labels_val = torch.cat((inputs_val[:,1:,:], labels_val), dim = 1)\n",
    "                loss_valid = loss_MSE(outputs_val, full_labels_val)\n",
    "\n",
    "            losses_valid.append(loss_valid.data)\n",
    "            losses_epoch_valid.append(loss_valid.data)\n",
    "            \n",
    "            # output\n",
    "            trained_number += 1\n",
    "            \n",
    "        avg_losses_epoch_train = sum(losses_epoch_train).cpu().numpy() / float(len(losses_epoch_train))\n",
    "        avg_losses_epoch_valid = sum(losses_epoch_valid).cpu().numpy() / float(len(losses_epoch_valid))\n",
    "        losses_epochs_train.append(avg_losses_epoch_train)\n",
    "        losses_epochs_valid.append(avg_losses_epoch_valid)\n",
    "        \n",
    "        # Early Stopping\n",
    "        if epoch == 0:\n",
    "            is_best_model = 1\n",
    "            best_model = model\n",
    "            min_loss_epoch_valid = 10000.0\n",
    "            if avg_losses_epoch_valid < min_loss_epoch_valid:\n",
    "                min_loss_epoch_valid = avg_losses_epoch_valid\n",
    "        else:\n",
    "            if min_loss_epoch_valid - avg_losses_epoch_valid > min_delta:\n",
    "                is_best_model = 1\n",
    "                best_model = model\n",
    "                min_loss_epoch_valid = avg_losses_epoch_valid \n",
    "                patient_epoch = 0\n",
    "            else:\n",
    "                is_best_model = 0\n",
    "                patient_epoch += 1\n",
    "                if patient_epoch >= patience:\n",
    "                    print('Early Stopped at Epoch:', epoch)\n",
    "                    break\n",
    "        \n",
    "        # Print training parameters\n",
    "        cur_time = time.time()\n",
    "        print('Epoch: {}, train_loss: {}, valid_loss: {}, time: {}, best model: {}'.format( \\\n",
    "                    epoch, \\\n",
    "                    np.around(avg_losses_epoch_train, decimals=8),\\\n",
    "                    np.around(avg_losses_epoch_valid, decimals=8),\\\n",
    "                    np.around([cur_time - pre_time] , decimals=2),\\\n",
    "                    is_best_model) )\n",
    "        pre_time = cur_time\n",
    "                \n",
    "    return best_model, [losses_train, losses_valid, losses_epochs_train, losses_epochs_valid]\n",
    "\n",
    "\n",
    "def Test_Model(model, test_dataloader, max_speed):\n",
    "    \n",
    "    if (type(model) == nn.modules.container.Sequential):\n",
    "        output_last = model[-1].output_last\n",
    "    else:\n",
    "        output_last = model.output_last\n",
    "    \n",
    "    inputs, labels = next(iter(test_dataloader))\n",
    "    [batch_size, type_size, step_size, fea_size] = inputs.size()\n",
    "\n",
    "    cur_time = time.time()\n",
    "    pre_time = time.time()\n",
    "    \n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    \n",
    "    loss_MSE = torch.nn.MSELoss()\n",
    "    loss_L1 = torch.nn.MSELoss()\n",
    "    \n",
    "    tested_batch = 0\n",
    "    \n",
    "    losses_mse = []\n",
    "    losses_l1 = [] \n",
    "    MAEs = []\n",
    "    MAPEs = []\n",
    "\n",
    "    \n",
    "    for data in test_dataloader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        if inputs.shape[0] != batch_size:\n",
    "            continue\n",
    "    \n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        else: \n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        outputs, imputed_outputs = model(inputs)\n",
    "    \n",
    "        loss_MSE = torch.nn.MSELoss()\n",
    "        loss_L1 = torch.nn.L1Loss()\n",
    "        \n",
    "        if output_last:\n",
    "            loss_mse = loss_MSE(torch.squeeze(outputs), torch.squeeze(labels))\n",
    "            loss_l1 = loss_L1(torch.squeeze(outputs), torch.squeeze(labels))\n",
    "            MAE = torch.mean(torch.abs(torch.squeeze(outputs) - torch.squeeze(labels)))\n",
    "            MAPE = torch.mean(torch.abs(torch.squeeze(outputs) - torch.squeeze(labels)) / torch.squeeze(labels))\n",
    "        else:\n",
    "            loss_mse = loss_MSE(outputs[:,-1,:], labels)\n",
    "            loss_l1 = loss_L1(outputs[:,-1,:], labels)\n",
    "            MAE = torch.mean(torch.abs(outputs[:,-1,:] - torch.squeeze(labels)))\n",
    "            MAPE = torch.mean(torch.abs(outputs[:,-1,:] - torch.squeeze(labels)) / torch.squeeze(labels))\n",
    "            c\n",
    "        losses_mse.append(loss_mse.data)\n",
    "        losses_l1.append(loss_l1.data)\n",
    "        MAEs.append(MAE.data)\n",
    "        MAPEs.append(MAPE.data)\n",
    "        \n",
    "        tested_batch += 1\n",
    "    \n",
    "        if tested_batch % 1000 == 0:\n",
    "            cur_time = time.time()\n",
    "            print('Tested #: {}, loss_l1: {}, loss_mse: {}, time: {}'.format( \\\n",
    "                  tested_batch * batch_size, \\\n",
    "                  np.around([loss_l1.data[0]], decimals=8), \\\n",
    "                  np.around([loss_mse.data[0]], decimals=8), \\\n",
    "                  np.around([cur_time - pre_time], decimals=8) ) )\n",
    "            pre_time = cur_time\n",
    "    losses_l1 = np.array(losses_l1)\n",
    "    losses_mse = np.array(losses_mse)\n",
    "    MAEs = np.array(MAEs)\n",
    "    MAPEs = np.array(MAPEs)\n",
    "    \n",
    "    mean_l1 = np.mean(losses_l1) * max_speed\n",
    "    std_l1 = np.std(losses_l1) * max_speed\n",
    "    MAE_ = np.mean(MAEs) * max_speed\n",
    "    MAPE_ = np.mean(MAPEs) * 100\n",
    "    \n",
    "    print('Tested: L1_mean: {}, L1_std: {}, MAE: {} MAPE: {}'.format(mean_l1, std_l1, MAE_, MAPE_))\n",
    "    return [losses_l1, losses_mse, mean_l1, std_l1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "devoted-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'loop'\n",
    "if data == 'inrix':\n",
    "    speed_matrix =  pd.read_pickle('../Data_Warehouse/Data_network_traffic/inrix_seattle_speed_matrix_2012')\n",
    "elif data == 'loop':\n",
    "    speed_matrix =  pd.read_pickle('data/speed_matrix_2015.pkl')\n",
    "\n",
    "train_dataloader, valid_dataloader, test_dataloader, max_speed, X_mean = PrepareDataset(speed_matrix, BATCH_SIZE = 64, masking = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "atlantic-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(train_dataloader, 'train_dataloader.pth')\n",
    "# torch.save(valid_dataloader, 'valid_dataloader.pth')\n",
    "# torch.save(test_dataloader, 'test_dataloader.pth')\n",
    "# torch.save(max_speed, 'max_speed.pth')\n",
    "# torch.save(X_mean, 'X_mean.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "impossible-rachel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = torch.load('train_dataloader.pth')\n",
    "# valid_dataloader = torch.load('valid_dataloader.pth')\n",
    "# test_dataloader = torch.load('test_dataloader.pth')\n",
    "# max_speed = torch.load('max_speed.pth')\n",
    "# X_mean = torch.load('X_mean.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exotic-pointer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4, 10, 323])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(train_dataloader).next()[0].shape # batch, (speed_sequences, X_last_obsv, Mask, Delta), seq_len, feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-intensity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(train_dataloader))\n",
    "[batch_size, type_size, step_size, fea_size] = inputs.size()\n",
    "input_dim = fea_size\n",
    "hidden_dim = fea_size\n",
    "output_dim = fea_size\n",
    "\n",
    "grud = GRUD(input_dim, hidden_dim, output_dim, X_mean, output_last = True)\n",
    "best_grud, losses_grud = Train_Model(grud, train_dataloader, valid_dataloader)\n",
    "[losses_l1, losses_mse, mean_l1, std_l1] = Test_Model(best_grud, test_dataloader, max_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "vocational-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item = iter(test_dataloader).next()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "differential-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, imputed = grud(test_item.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "national-quantity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5085, 0.5267, 0.5261,  ..., 0.6613, 0.6110, 0.6085],\n",
       "         [0.5084, 0.0000, 0.0000,  ..., 0.6396, 0.5788, 0.6008],\n",
       "         [0.5215, 0.5219, 0.5104,  ..., 0.6415, 0.6062, 0.6073],\n",
       "         ...,\n",
       "         [0.5192, 0.5390, 0.0000,  ..., 0.6166, 0.5867, 0.6087],\n",
       "         [0.5166, 0.5083, 0.4953,  ..., 0.6392, 0.5928, 0.5923],\n",
       "         [0.5095, 0.5323, 0.5323,  ..., 0.0000, 0.5767, 0.0000]],\n",
       "\n",
       "        [[0.6481, 0.6823, 0.6782,  ..., 0.6734, 0.6510, 0.6511],\n",
       "         [0.6574, 0.6937, 0.6908,  ..., 0.6774, 0.6740, 0.6411],\n",
       "         [0.0000, 0.6830, 0.6710,  ..., 0.6773, 0.6399, 0.0000],\n",
       "         ...,\n",
       "         [0.6549, 0.6886, 0.0000,  ..., 0.6776, 0.6253, 0.6614],\n",
       "         [0.6410, 0.6827, 0.7142,  ..., 0.6876, 0.6657, 0.6483],\n",
       "         [0.6594, 0.6899, 0.6789,  ..., 0.6790, 0.6523, 0.6533]],\n",
       "\n",
       "        [[0.6236, 0.6493, 0.0000,  ..., 0.6441, 0.6024, 0.6158],\n",
       "         [0.6218, 0.6692, 0.6808,  ..., 0.6543, 0.6368, 0.6266],\n",
       "         [0.6041, 0.6651, 0.6387,  ..., 0.6797, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.6019, 0.6024, 0.6248,  ..., 0.0000, 0.0000, 0.6139],\n",
       "         [0.5958, 0.6305, 0.0000,  ..., 0.6444, 0.6253, 0.0000],\n",
       "         [0.0000, 0.6662, 0.6454,  ..., 0.0000, 0.6333, 0.6393]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5806, 0.6054, 0.6338,  ..., 0.6894, 0.6523, 0.6568],\n",
       "         [0.5806, 0.0000, 0.6338,  ..., 0.6946, 0.6825, 0.6677],\n",
       "         [0.5806, 0.6054, 0.6338,  ..., 0.6761, 0.6483, 0.6496],\n",
       "         ...,\n",
       "         [0.5806, 0.6054, 0.6338,  ..., 0.6936, 0.6566, 0.6607],\n",
       "         [0.5806, 0.6054, 0.6338,  ..., 0.0000, 0.6150, 0.0000],\n",
       "         [0.5806, 0.6054, 0.6338,  ..., 0.6724, 0.6493, 0.6306]],\n",
       "\n",
       "        [[0.5810, 0.6012, 0.0000,  ..., 0.6672, 0.6061, 0.5931],\n",
       "         [0.0000, 0.5624, 0.5190,  ..., 0.0000, 0.5808, 0.5938],\n",
       "         [0.5534, 0.0000, 0.5440,  ..., 0.0000, 0.0000, 0.5816],\n",
       "         ...,\n",
       "         [0.5368, 0.5398, 0.0000,  ..., 0.6639, 0.0000, 0.6058],\n",
       "         [0.0000, 0.5563, 0.5176,  ..., 0.6619, 0.5961, 0.5981],\n",
       "         [0.5535, 0.0000, 0.5835,  ..., 0.0000, 0.6178, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.5708,  ..., 0.6973, 0.6408, 0.6464],\n",
       "         [0.5369, 0.5544, 0.5689,  ..., 0.6932, 0.6341, 0.6243],\n",
       "         [0.5568, 0.5550, 0.5675,  ..., 0.6694, 0.6043, 0.6317],\n",
       "         ...,\n",
       "         [0.0000, 0.6020, 0.5943,  ..., 0.6419, 0.6130, 0.5831],\n",
       "         [0.0000, 0.0000, 0.5668,  ..., 0.6776, 0.6263, 0.6361],\n",
       "         [0.5294, 0.5338, 0.5366,  ..., 0.6452, 0.6186, 0.0000]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_item[:,0,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "wooden-burden",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5085, 0.5267, 0.5261,  ..., 0.6613, 0.6110, 0.6085],\n",
       "         [0.5084, 0.5267, 0.5261,  ..., 0.6396, 0.5788, 0.6008],\n",
       "         [0.5215, 0.5219, 0.5104,  ..., 0.6415, 0.6062, 0.6073],\n",
       "         ...,\n",
       "         [0.5192, 0.5390, 0.4726,  ..., 0.6166, 0.5867, 0.6087],\n",
       "         [0.5166, 0.5083, 0.4953,  ..., 0.6392, 0.5928, 0.5923],\n",
       "         [0.5095, 0.5323, 0.5323,  ..., 0.6311, 0.5767, 0.5923]],\n",
       "\n",
       "        [[0.6481, 0.6823, 0.6782,  ..., 0.6734, 0.6510, 0.6511],\n",
       "         [0.6574, 0.6937, 0.6908,  ..., 0.6774, 0.6740, 0.6411],\n",
       "         [0.6540, 0.6830, 0.6710,  ..., 0.6773, 0.6399, 0.6411],\n",
       "         ...,\n",
       "         [0.6549, 0.6886, 0.6725,  ..., 0.6776, 0.6253, 0.6614],\n",
       "         [0.6410, 0.6827, 0.7142,  ..., 0.6876, 0.6657, 0.6483],\n",
       "         [0.6594, 0.6899, 0.6789,  ..., 0.6790, 0.6523, 0.6533]],\n",
       "\n",
       "        [[0.6236, 0.6493, 0.0000,  ..., 0.6441, 0.6024, 0.6158],\n",
       "         [0.6218, 0.6692, 0.6808,  ..., 0.6543, 0.6368, 0.6266],\n",
       "         [0.6041, 0.6651, 0.6387,  ..., 0.6797, 0.6368, 0.6266],\n",
       "         ...,\n",
       "         [0.6019, 0.6024, 0.6248,  ..., 0.6530, 0.6526, 0.6139],\n",
       "         [0.5958, 0.6305, 0.6248,  ..., 0.6444, 0.6253, 0.6139],\n",
       "         [0.5934, 0.6662, 0.6454,  ..., 0.6360, 0.6333, 0.6393]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5806, 0.6054, 0.6338,  ..., 0.6894, 0.6523, 0.6568],\n",
       "         [0.5806, 0.6054, 0.6338,  ..., 0.6946, 0.6825, 0.6677],\n",
       "         [0.5806, 0.6054, 0.6338,  ..., 0.6761, 0.6483, 0.6496],\n",
       "         ...,\n",
       "         [0.5806, 0.6054, 0.6338,  ..., 0.6936, 0.6566, 0.6607],\n",
       "         [0.5806, 0.6054, 0.6338,  ..., 0.6818, 0.6150, 0.6607],\n",
       "         [0.5806, 0.6054, 0.6338,  ..., 0.6724, 0.6493, 0.6306]],\n",
       "\n",
       "        [[0.5810, 0.6012, 0.0000,  ..., 0.6672, 0.6061, 0.5931],\n",
       "         [0.5789, 0.5624, 0.5190,  ..., 0.6574, 0.5808, 0.5938],\n",
       "         [0.5534, 0.5624, 0.5440,  ..., 0.6567, 0.5808, 0.5816],\n",
       "         ...,\n",
       "         [0.5368, 0.5398, 0.5295,  ..., 0.6639, 0.5832, 0.6058],\n",
       "         [0.5355, 0.5563, 0.5176,  ..., 0.6619, 0.5961, 0.5981],\n",
       "         [0.5535, 0.5563, 0.5835,  ..., 0.6523, 0.6178, 0.5981]],\n",
       "\n",
       "        [[0.0109, 0.0000, 0.5708,  ..., 0.6973, 0.6408, 0.6464],\n",
       "         [0.5369, 0.5544, 0.5689,  ..., 0.6932, 0.6341, 0.6243],\n",
       "         [0.5568, 0.5550, 0.5675,  ..., 0.6694, 0.6043, 0.6317],\n",
       "         ...,\n",
       "         [0.5385, 0.6020, 0.5943,  ..., 0.6419, 0.6130, 0.5831],\n",
       "         [0.5390, 0.6020, 0.5668,  ..., 0.6776, 0.6263, 0.6361],\n",
       "         [0.5294, 0.5338, 0.5366,  ..., 0.6452, 0.6186, 0.6361]]],\n",
       "       device='cuda:0', grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-shelter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
